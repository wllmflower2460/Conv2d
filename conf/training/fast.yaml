# Fast training configuration for quick experiments
# @package training

epochs: 50
gradient_accumulation_steps: 1

# Faster optimization
optimizer:
  type: adamw
  betas: [0.9, 0.98]
  eps: 1e-8
  amsgrad: true

# Aggressive learning rate schedule
scheduler:
  type: onecycle
  max_lr: 5e-3
  pct_start: 0.3
  anneal_strategy: cos
  
# Minimal regularization for speed
regularization:
  dropout: 0.0
  weight_decay: 1e-5
  label_smoothing: 0.0
  gradient_clip: 5.0
  
# Efficient data loading
dataloader:
  shuffle_train: true
  shuffle_val: false
  drop_last: false
  
# Minimal checkpointing
checkpointing:
  save_best: true
  save_last: false
  save_interval: null
  keep_last_k: 1
  monitor: val_loss
  mode: min
  
# Aggressive mixed precision
mixed_precision:
  enabled: true
  opt_level: O2
  loss_scale: 128