# Standard training configuration
# @package training

epochs: 100
gradient_accumulation_steps: 1

# Optimization
optimizer:
  type: adam
  betas: [0.9, 0.999]
  eps: 1e-8
  amsgrad: false

# Learning rate schedule
scheduler:
  type: cosine
  warmup_epochs: 5
  warmup_method: linear
  min_lr_ratio: 0.01
  
# Regularization
regularization:
  dropout: 0.1
  weight_decay: 1e-4
  label_smoothing: 0.1
  gradient_clip: 1.0
  
# Data loading
dataloader:
  shuffle_train: true
  shuffle_val: false
  drop_last: true
  
# Checkpointing
checkpointing:
  save_best: true
  save_last: true
  save_interval: 10  # epochs
  keep_last_k: 3
  monitor: val_loss
  mode: min
  
# Mixed precision training
mixed_precision:
  enabled: ${hardware.mixed_precision}
  opt_level: O1  # O0, O1, O2, O3
  loss_scale: dynamic