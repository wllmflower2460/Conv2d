# T0.5 Resubmission Action Plan
**Due Date**: October 1, 2025 (7 days)  
**Status**: CONDITIONAL PASS (3.6/5) - Critical fixes required

---

## ðŸ”´ Critical Blockers (Must Fix)

### 1. FSQ Ablation on Real Data (Day 1-2)
**Issue**: Synthetic data validation (100% accuracy) insufficient  
**Action Required**:
```bash
# Location: scripts/training/train_fsq_real_data_m13.py
# Task: Run ablation on real PAMAP2 with proper splits
python scripts/training/train_fsq_real_data_m13.py \
    --dataset PAMAP2 \
    --split temporal \
    --train_ratio 0.6 \
    --val_ratio 0.2 \
    --test_ratio 0.2
```

**Success Criteria**:
- [ ] Temporal train/test splits (no leakage)
- [ ] n>30 samples per condition
- [ ] Cohen's d > 0.5 effect size
- [ ] Bonferroni correction (Î± = 0.0083)

### 2. Scope Decision: Human-Only vs Cross-Species (Day 1)
**Issue**: Only n=287 quadruped sequences (need n>1000)  
**Recommended Action**: **Rescope to human-only for T0.5**

**Options**:
- **Option A (Recommended)**: Human-only validation
  - Faster gate passage (1 week)
  - Focus on discrete state hypothesis
  - Position quadruped as future work
  
- **Option B**: Acquire more quadruped data
  - Need n>1000 sequences
  - 2-3 week delay
  - Partner with veterinary lab

### 3. Fix HDP Integration (Day 3-4)
**Issue**: FSQ alone: 100% â†’ FSQ+HDP: 48.3% (52% drop!)  
**Location**: `models/conv2d_vq_hdp_hsmm.py`

**Debug Steps**:
```python
# Check gradient flow
def debug_hdp_integration():
    # 1. Visualize gradients through FSQâ†’HDP
    # 2. Test FSQâ†’HSMM directly (bypass HDP)
    # 3. Ablate HDP components
    # 4. Decision: fix or remove HDP
```

### 4. Optimize Codebook Size (Day 3)
**Issue**: 7.4% utilization with 512 codes (need ~32-64)  
**Location**: `models/conv2d_vq_model.py`

**Implementation**:
```python
codebook_sizes = [8, 16, 32, 64, 128]
for size in codebook_sizes:
    model = Conv2dVQModel(num_codes=size)
    # Train and evaluate
    # Select minimum size maintaining >70% accuracy
```

### 5. Implement Real MI Computation (Day 4-5)
**Issue**: Placeholder transfer entropy returns random values  
**Location**: `models/t0_complete_implementation.py:142-154`

**Fix Required**:
```python
# Replace placeholder with actual implementation
from jpype import startJVM, getDefaultJVMPath
import jpype.imports
from infodynamics.measures import TransferEntropyCalculatorKraskov

def compute_te_actual(source, target, k=5, delay=1):
    # Use JIDT or implement k-NN estimator
    calc = TransferEntropyCalculatorKraskov()
    calc.setProperty("k", str(k))
    calc.initialise(1, 1)
    calc.setObservations(source, target)
    return calc.computeAverageLocalOfObservations()
```

---

## ðŸ“Š Statistical Requirements (Day 5-6)

### Missing Elements to Add:
```python
statistical_fixes = {
    'bonferroni': {
        'num_tests': 6,
        'alpha_adjusted': 0.05/6,  # 0.0083
        'apply_to': 'all hypothesis tests'
    },
    'effect_sizes': {
        'cohen_d': 'for all comparisons',
        'eta_squared': 'for ANOVA',
        'confidence_intervals': '95% CI'
    },
    'cross_validation': {
        'k_folds': 5,
        'report': 'mean Â± std',
        'stability': 'CV < 0.1'
    }
}
```

---

## ðŸ“ Documentation Requirements (Day 6)

### 1. Expert Evaluation Protocol
**Create**: `docs/theory/expert_evaluation_protocol.md`
- Define expert qualifications
- Document evaluation rubric
- Include inter-rater reliability (IRR)

### 2. Limitations Document
**Create**: `docs/theory/limitations_and_scope.md`
- Behavioral vs kinematic clarity
- Data limitations acknowledged
- Future work clearly defined

---

## ðŸš€ Daily Schedule

### Day 1 (Sep 25): Foundation
- [ ] Morning: Scope decision (human-only recommended)
- [ ] Afternoon: Start real PAMAP2 ablation
- [ ] Evening: Set up statistical analysis pipeline

### Day 2 (Sep 26): Validation
- [ ] Complete FSQ ablation on real data
- [ ] Run statistical tests with Bonferroni
- [ ] Calculate effect sizes (Cohen's d)

### Day 3 (Sep 27): Optimization
- [ ] Morning: Debug HDP integration
- [ ] Afternoon: Codebook size optimization (32-64)
- [ ] Evening: Test optimized configuration

### Day 4 (Sep 28): Implementation
- [ ] Morning: Implement actual MI computation
- [ ] Afternoon: Replace placeholder TE
- [ ] Evening: Validate MI/TE results

### Day 5 (Sep 29): Analysis
- [ ] Morning: Complete statistical analysis
- [ ] Afternoon: Generate visualizations
- [ ] Evening: Cross-validation stability

### Day 6 (Sep 30): Documentation
- [ ] Morning: Expert evaluation protocol
- [ ] Afternoon: Update submission package
- [ ] Evening: Final testing and validation

### Day 7 (Oct 1): Submission
- [ ] Morning: Final review and checks
- [ ] Afternoon: Compile resubmission package
- [ ] Evening: **Submit T0.5 resubmission**

---

## ðŸ“¦ Resubmission Deliverables

### Required Files:
```
T0.5_Resubmission/
â”œâ”€â”€ RESPONSE_TO_COMMITTEE.md           # Address each concern
â”œâ”€â”€ REAL_DATA_VALIDATION/
â”‚   â”œâ”€â”€ pamap2_ablation_results.json  # Real data results
â”‚   â”œâ”€â”€ statistical_analysis.ipynb    # With effect sizes
â”‚   â””â”€â”€ confusion_matrices/           # Per-class performance
â”œâ”€â”€ OPTIMIZATIONS/
â”‚   â”œâ”€â”€ codebook_optimization.json    # 32 vs 64 vs 512
â”‚   â”œâ”€â”€ hdp_fix_or_removal.md        # Decision and rationale
â”‚   â””â”€â”€ mi_implementation.py         # Actual von Mises MI
â”œâ”€â”€ DOCUMENTATION/
â”‚   â”œâ”€â”€ expert_protocol.md           # IRR metrics
â”‚   â”œâ”€â”€ limitations.md               # Clear scope
â”‚   â””â”€â”€ future_work.md              # Quadruped plans
â””â”€â”€ CODE_FIXES/
    â”œâ”€â”€ models/                      # Updated implementations
    â””â”€â”€ scripts/                     # Validation scripts
```

---

## âš ï¸ Risk Mitigation

### If HDP Unfixable by Day 3:
- Remove HDP component
- Proceed with FSQ+HSMM only
- Document theoretical implications

### If Real Data Validation <70%:
- Analyze failure modes
- Adjust hyperparameters
- Consider 1-week extension request

### If Codebook <64 Still Works:
- Use smallest viable size
- Document memory/accuracy tradeoff
- Plan further optimization for D1

---

## ðŸ“ž Committee Response Prep

### Key Messages:
1. "Real PAMAP2 validation complete with proper statistical rigor"
2. "Rescoped to human-only for robust validation"
3. "HDP removed/fixed based on empirical evidence"
4. "Codebook optimized to 32/64 codes (from 512)"
5. "All statistical requirements met (Bonferroni, effect sizes, CV)"

### Anticipated Questions:
- **Q: Why rescope to human-only?**
  - A: "Focus on proving core discrete state hypothesis first. Quadruped positioned as validated future work with pilot results."

- **Q: Why remove HDP?**
  - A: "Empirical testing showed FSQ+HSMM sufficient. HDP added complexity without performance benefit."

- **Q: Is 32 codes enough?**
  - A: "Effective dimensionality analysis shows 32-64 codes optimal. Maintains >70% accuracy with 16x size reduction."

---

## âœ… Success Criteria

### Minimum for PASS:
- [ ] Real data validation >70% accuracy
- [ ] Proper statistical rigor (Bonferroni, effect sizes)
- [ ] Codebook optimized (32-64 codes)
- [ ] HDP issue resolved
- [ ] Actual MI implementation

### Target for Strong PASS:
- [ ] Real data validation >75% accuracy
- [ ] Cross-validation stability (CV < 0.1)
- [ ] Complete documentation
- [ ] All code properly tested
- [ ] Clear future work plan

---

## ðŸŽ¯ Next Immediate Actions

1. **NOW**: Make scope decision (human-only recommended)
2. **TODAY 2PM**: Start real PAMAP2 ablation
3. **TODAY 5PM**: Set up statistical pipeline
4. **TOMORROW 9AM**: Review initial ablation results

**Command to start**:
```bash
cd /home/wllmflower/Development/Conv2d
python scripts/training/train_fsq_real_data_m13.py --config configs/improved_config.py
```

---

*Action Plan Created: 2025-09-24*  
*Resubmission Due: 2025-10-01*  
*Contact: wllmflower@gmail.com*